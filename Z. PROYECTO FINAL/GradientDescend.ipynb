{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crean dos tensores aleatorios: A, una matriz de tamaño 5x5, y x, un vector de tamaño 5x1. Estos tensores se inicializan con valores aleatorios utilizando la función \"torch.rand\".\n",
    "\n",
    "A continuación, se realiza la multiplicación matricial entre A y x utilizando el operador \"@\", que es el operador de multiplicación de matrices en PyTorch. El resultado se asigna a la variable \"y\".\n",
    "\n",
    "Se define la tasa de aprendizaje con el valor de 0.01. Esta tasa de aprendizaje se utiliza para actualizar los valores de x en cada iteración del descenso de gradiente.\n",
    "\n",
    "Se define la función \"grad(x)\" que calcula el gradiente del error cuadrático medio con respecto a x. En este caso, se utiliza la fórmula del gradiente para el problema de mínimos cuadrados: 2 * A^T * (A * x - y), donde \"T\" denota la transposición de A.\n",
    "\n",
    "A continuación, se realiza un bucle \"for\" con 1000 iteraciones. En cada iteración, se actualiza el valor de x utilizando la regla de actualización del descenso de gradiente: x = x - learning_rate * grad(x).\n",
    "\n",
    "Después de actualizar x, se aplica un \"torch.clamp\". Esta operación limita los valores de x dentro del rango de -1 a 1. Esto puede ser útil para restringir los valores de x dentro de un rango específico.\n",
    "\n",
    "Finalmente, se imprimen los resultados. La expresión \"(A * x) - y\" muestra el error residual después de ajustar los valores de x. La variable \"x\" muestra los valores de x después de las iteraciones del descenso de gradiente. Y la expresión \"torch.norm(x, 1)\" calcula la norma L1 del vector x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (40x100 and 5x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[113], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mA\u001b[39m.\u001b[39mT\u001b[39m@\u001b[39m(A\u001b[39m@x\u001b[39m\u001b[39m-\u001b[39my)\n\u001b[0;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5000\u001b[39m):\n\u001b[1;32m---> 23\u001b[0m     x\u001b[39m=\u001b[39mx\u001b[39m-\u001b[39mlearning_rate\u001b[39m*\u001b[39mgrad(x)\n\u001b[0;32m     25\u001b[0m     x\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mclamp(x,\u001b[39m-\u001b[39mbeta,beta)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m((A\u001b[39m@x\u001b[39m)\u001b[39m-\u001b[39my)\n",
      "Cell \u001b[1;32mIn[113], line 20\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrad\u001b[39m(x):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mA\u001b[39m.\u001b[39mT\u001b[39m@\u001b[39m(A\u001b[39m@x\u001b[39;49m\u001b[39m-\u001b[39my)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (40x100 and 5x1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 10\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Realice un anális de la convergencia del algoritmo variando los par´ametros β y el learning rate. (5 veces cada uno)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 20\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 30\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 40\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 50\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 60\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.01\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 10\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.001\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 10\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.0001\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 10\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.00001\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "A   = torch.rand(40,100)\n",
    "x_0 = torch.rand(100,1)\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 5 elementos diferentes de cero.\n",
    "x_0[5:,:] = 0\n",
    "\n",
    "# convertir el vector x_0  en un vector sparse con 10 elementos diferentes de cero.\n",
    "x_0[10:,:] = 0\n",
    "\n",
    "beta = 10\n",
    "\n",
    "y = A@x_0 #a por x\n",
    "\n",
    "learning_rate=0.000001\n",
    "\n",
    "def grad(x):\n",
    "    return 2*A.T@(A@x-y)\n",
    "\n",
    "for i in range(5000):\n",
    "    x=x-learning_rate*grad(x)\n",
    "\n",
    "    x=torch.clamp(x,-beta,beta)\n",
    "    \n",
    "        \n",
    "        \n",
    "print((A@x)-y)\n",
    "print(x)\n",
    "print(torch.norm(x,1))\n",
    "print(torch.norm(y- A@x,1))\n",
    "print(torch.norm(x- x_0,2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
